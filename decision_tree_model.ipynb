{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abalone.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4177\n"
     ]
    }
   ],
   "source": [
    "N = len(data)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate impurity in data set with given impurity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_set_impurity(classes, impurity_measure):\n",
    "    prob_distribution = get_prob_distr(classes)\n",
    "    impurity = 0\n",
    "    \n",
    "    if impurity_measure == 'entropy':\n",
    "        for x in prob_distribution:\n",
    "            impurity -= x*math.log(x, 2)\n",
    "            \n",
    "    elif impurity_measure == 'gini':\n",
    "        for x in prob_distribution:\n",
    "            impurity += x*(1-x)\n",
    "            \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the probability distribution of the unique values in given array\n",
    "def get_prob_distr(classes):\n",
    "    cl, prob = np.unique(classes, return_counts=True)\n",
    "    return prob/len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_gain(arr, split_point, col, entropy_before, discrete, impurity_measure):    \n",
    "    if discrete:\n",
    "        split_arr = split(arr, arr[:,col] == split_point)\n",
    "    else: \n",
    "        split_arr = split(arr, arr[:,col] < split_point)\n",
    "        \n",
    "    left, right = split_arr[0], split_arr[1]\n",
    "        \n",
    "    entropy_after = find_set_impurity(left[:,-1], impurity_measure)*(len(left)/len(arr))\n",
    "    entropy_after += find_set_impurity(right[:,-1], impurity_measure)*(len(right)/len(arr))\n",
    "    \n",
    "    return entropy_before - entropy_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(arr, cond):\n",
    "    return [arr[cond], arr[~cond]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_split_feature(arr, max_split_checks, impurity_measure):\n",
    "    best_IG = 0\n",
    "    best_feature = -1\n",
    "    split_point = None\n",
    "    entropy_before = find_set_impurity(arr[:,-1], impurity_measure)\n",
    "    \n",
    "    for feature in range(arr.shape[1]-1): # last column is the label column, all others are features\n",
    "        feature_IG, ig, best_feature_split = 0, 0, None\n",
    "        unique = np.unique(arr[:,feature])\n",
    "        \n",
    "        if (type(arr[0,feature]) == str):\n",
    "            # try splitting into 1 | all others, for every type\n",
    "            for u in unique:\n",
    "                ig = get_info_gain(arr, u, feature, entropy_before, True, impurity_measure)\n",
    "                if ig > feature_IG: \n",
    "                    feature_IG = ig\n",
    "                    best_feature_split = u\n",
    "            \n",
    "        else:\n",
    "            if len(unique) <= max_split_checks+1: # try split between every unique value (ascending)\n",
    "                for i in range(len(unique)-1):\n",
    "                    split_point = (unique[i] + unique[i+1])/2\n",
    "                    ig = get_info_gain(arr, split_point, feature, entropy_before, False, impurity_measure)\n",
    "                    if ig > feature_IG: \n",
    "                        feature_IG = ig\n",
    "                        best_feature_split = split_point\n",
    "                \n",
    "            else: # try max_split_checks splits uniformly between min and max of feature\n",
    "                step = (np.max(arr[:,feature])-np.min(arr[:,feature]))/max_split_checks\n",
    "                for i in range(1, max_split_checks):\n",
    "                    split_point = np.min(arr[:,feature])+i*step\n",
    "                    ig = get_info_gain(arr, split_point, feature, entropy_before, False, impurity_measure)\n",
    "                    if ig > feature_IG: \n",
    "                        feature_IG = ig\n",
    "                        best_feature_split = split_point\n",
    "                        \n",
    "        if feature_IG > best_IG: \n",
    "            best_IG = feature_IG\n",
    "            best_feature = feature\n",
    "            best_split_point = best_feature_split\n",
    "    \n",
    "    if best_feature == -1: raise Exception('Could not find a feature to split')\n",
    "    return best_feature, best_split_point\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decision_tree():\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.n_nodes = 1\n",
    "        \n",
    "    def learn(self, X, y, impurity_measure='entropy', max_split_checks=35, prune=False, prune_split=0):\n",
    "        self.impurity_measure = impurity_measure\n",
    "        self.max_split_checks = max_split_checks\n",
    "        \n",
    "        pr = None;\n",
    "        if prune: # split data into training and pruning sets\n",
    "            lim = math.ceil(len(X)*(1-prune_split))\n",
    "            arr = np.column_stack((X[:lim,:],y[:lim]))\n",
    "            pr = np.column_stack((X[lim:,:],y[lim:]))\n",
    "        else:\n",
    "            arr = np.column_stack((X,y))\n",
    "        \n",
    "        self.root = decision_tree.node()\n",
    "        self.build(arr, self.root)\n",
    "        \n",
    "        if prune: self.prune(pr)\n",
    "        \n",
    "    def build(self, arr, current_node):\n",
    "        if len(arr) < 1: raise Exception('Recursive call to form node with empty dataframe')\n",
    "        current_node.data = arr # store for use in pruning\n",
    "        if len(np.unique(arr[:,-1])) == 1:\n",
    "            # all data points have the same label, return a leaf with that label\n",
    "            current_node.label = arr[0,-1]\n",
    "        else:\n",
    "            identical_features = True\n",
    "            for feature in range(arr.shape[1]-1):\n",
    "                if len(np.unique(arr[:,feature])) > 1:\n",
    "                    identical_features = False\n",
    "                    break\n",
    "                    \n",
    "            if identical_features:\n",
    "                # all remaining data points have identical features, set the most common label\n",
    "                current_node.label = stats.mode(X[:,-1])[0][0]\n",
    "                \n",
    "            else: # find a feature and value to split the data set\n",
    "                feature, split_val = find_split_feature(arr, self.max_split_checks, self.impurity_measure)\n",
    "                if type(arr[0,feature]) == str:\n",
    "                    current_node.continuous = False\n",
    "                    split_arr = split(arr, arr[:,feature] == split_val)\n",
    "                else: \n",
    "                    current_node.continuous = True\n",
    "                    split_arr = split(arr, arr[:,feature] < split_val)\n",
    "                \n",
    "                current_node.left = decision_tree.node()\n",
    "                current_node.right = decision_tree.node()\n",
    "                current_node.split = split_val\n",
    "                current_node.feature = feature\n",
    "                self.n_nodes += 2\n",
    "                self.build(split_arr[0], current_node.left)\n",
    "                self.build(split_arr[1], current_node.right)\n",
    "                    \n",
    "    def predict(self, x):\n",
    "        current_node = self.root\n",
    "        while current_node.label is None:\n",
    "            if current_node.continuous:\n",
    "                if x[current_node.feature] < current_node.split:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "            else:\n",
    "                if x[current_node.feature] == current_node.split:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "        return current_node.label\n",
    "    \n",
    "    def prune(self, pr):\n",
    "        queue = []\n",
    "        queue.append(self.root)\n",
    "        acc = accuracy(self, pr[:,:-1], pr[:,-1]) # accuracy before pruning\n",
    "        \n",
    "        while queue:\n",
    "            n = queue.pop() # current node\n",
    "            left, right = n.left, n.right # subtree\n",
    "            n.label = stats.mode(n.data[:,-1])[0][0] # most common label in subtree\n",
    "            # when a label is set at this node, the predict method will stop here and ignore the subtree\n",
    "            new_acc = accuracy(self, pr[:,:-1], pr[:,-1]) \n",
    "            \n",
    "            if new_acc >= acc:\n",
    "                n.left, n.right = None, None\n",
    "            else:\n",
    "                n.label = None\n",
    "                queue.append(n.left)\n",
    "                queue.append(n.right)\n",
    "    \n",
    "    def get_n_nodes(self):\n",
    "        return self.n_nodes\n",
    "    \n",
    "    class node():\n",
    "        def __init__(self):\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.continuous = None\n",
    "            self.label = None\n",
    "            self.split = None\n",
    "            self.feature = None\n",
    "            self.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class id3_model():\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        \n",
    "    def learn(self, X, y, impurity_measure='entropy', prune=False):\n",
    "        self.tree = decision_tree()\n",
    "        self.tree.learn(X, y, impurity_measure=impurity_measure, prune=prune, prune_split=0.1)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.tree is None: raise Exception('Attempting to predict with untrained model.')\n",
    "        else: return self.tree.predict(X)\n",
    "        \n",
    "    def get_n_nodes(self):\n",
    "        return self.tree.get_n_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X_test, y_test):\n",
    "    accuracy = 0\n",
    "    for i in range(len(X_test)):\n",
    "        if model.predict(X_test[i]) == y_test[i]:\n",
    "            accuracy += 1\n",
    "        \n",
    "    return (accuracy/len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data set into training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3759.3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data[:3760,:-1]\n",
    "y = data[:3760,-1]\n",
    "X_test = data[3760:,:-1]\n",
    "y_test = data[3760:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and test the model with entropy as impurity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23261390887290168\n",
      "Learning time:   7.0s\n"
     ]
    }
   ],
   "source": [
    "model = id3_model()\n",
    "start = time.time()\n",
    "model.learn(X, y)\n",
    "end = time.time()\n",
    "print(accuracy(model, X_test, y_test))\n",
    "print(\"Learning time: % 5.1fs\" %(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn and test the model with Gini as impurity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2158273381294964\n"
     ]
    }
   ],
   "source": [
    "model = id3_model()\n",
    "model.learn(X, y, impurity_measure='gini')\n",
    "print(accuracy(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_n_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3679"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = id3_model()\n",
    "model.learn(X, y, prune=True)\n",
    "model.get_n_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to sklearn's DecisionTreeClassifier\n",
    "I had to use a label encoder, as the DecisionTreeClassifier did not accept mixed data types as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data[:,0])\n",
    "tr_data = le.transform(data[:,0])\n",
    "tr_data = np.column_stack((tr_data,data[:,1:]))\n",
    "X = tr_data[:3760,:-1]\n",
    "y = tr_data[:3760,-1]\n",
    "y = y.astype('int')\n",
    "X_test = tr_data[3760:,:-1]\n",
    "y_test = tr_data[3760:,-1]\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21342925659472423\n",
      "Learning time:   0.0s\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "start = time.time()\n",
    "clf.fit(X,y)\n",
    "end = time.time()\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"Learning time: % 5.1fs\" %(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
